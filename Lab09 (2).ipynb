{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4hVv9ARzFwg"
      },
      "source": [
        "#Overview\n",
        "In this lab, we first have a brief introduction to data mining, the tools and the dataset. Then we will use Python to perform data mining tasks, like association rule mining and mining frequent itemsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUVo_pG-zOQ_"
      },
      "source": [
        "#Background\n",
        "Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. It is an essential process where intelligent methods are applied to extract data patterns. It is an interdisciplinary subfield of computer science.\n",
        "\n",
        "The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the “knowledge discovery in databases” process (KDD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb_FHMfTz0MA"
      },
      "source": [
        "#Data Mining Tools in the Market\n",
        "There are many data mining tools available in the market today. Some of them are free and open-source, while others are proprietary and commercial. Those tools could be used for data mining, machine learning, and data visualization without or with only minimal programming knowledge. They are used for business and commercial applications as well as for research, education, training, rapid prototyping, and application development and supports all steps of the machine learning process including data preparation, results visualization, model validation, and optimization\n",
        "\n",
        "Let's take a look to some of the most popular data mining tools listed below.\n",
        "\n",
        "###[Orange](https://orange.biolab.si/)\n",
        "Orange is a component-based data mining and machine learning software suite written in the Python programming language. It features a visual programming front-end for explorative rapid qualitative data analysis and interactive data visualization. It allows users to create data analysis workflows, assemble and run them, and visualize the obtained data and intermediate results cooperatively with Python code. Orange is free software released under the terms of the GNU General Public License. Orange is cross-platform and works on Windows, macOS, and Linux. It can be installed in a Python virtual environment via pip package manager or conda package and environment manager.\n",
        "\n",
        "###[RapidMiner](https://rapidminer.com/)\n",
        "RapidMiner is a data science software platform developed by the company of the same name that provides an integrated environment for data preparation, machine learning, deep learning, text mining, and predictive analytics. It is used for business and commercial applications as well as for research, education, training, rapid prototyping, and application development and supports all steps of the machine learning process including data preparation, results visualization, model validation, and optimization.\n",
        "\n",
        "###[Weka](https://www.cs.waikato.ac.nz/ml/weka/)\n",
        "Weka is a Java based tools and collection of machine learning algorithms for data mining tasks. It contains tools for data preparation, classification, regression, clustering, association rules mining, and visualization. Found only on the islands of New Zealand, the Weka is a flightless bird with an inquisitive nature. The name is pronounced like this, and the bird sounds like this. Weka is open-source software issued under the GNU General Public License.\n",
        "\n",
        "###[KNIME](https://www.knime.com/)\n",
        "KNIME is a free and open-source data analytics, reporting, and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis, and visualization without, or with only minimal, programming. To some extent as advanced analytics tool KNIME can be considered as a SAS alternative.\n",
        "\n",
        "\n",
        "The above are very commonly used and practical data mining tools, but in this lab, we will teach you how to directly use python code to complete data mining work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n82A5QnC0Z-w"
      },
      "source": [
        "#Case Study: Foodmart Dataset\n",
        "Food Mart is a grocery store chain that has stores in many cities across the United States. Foodmart dataset is a dataset of transactions from the grocery store. It contains 2,000 transactions with 1,000 items. It is available in the basket format from https://datasets.biolab.si/core/foodmart.basket. It’s a text file with one transaction per line. Each transaction is a list of items separated by commas. The first item in each transaction is the transaction ID. The rest of the items are the items in the transaction. The items are separated by commas. The transaction ID and the items are separated by =.\n",
        "\n",
        "We initially download the dataset using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nffv_TD70dAd",
        "outputId": "24f21a39-5f5e-4b37-8d91-25da6bfa6534"
      },
      "outputs": [],
      "source": [
        "!curl https://datasets.biolab.si/core/foodmart.basket -o foodmart.basket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bemck7uo2GBW"
      },
      "source": [
        "To verify the file's format, we print the information from the first 10 lines. Each line represents a transaction record, with STORE_ID_X indicating the supermarket number at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pczRHkOr2B6c",
        "outputId": "2031224f-e6b9-4ef0-e841-1eaaf1f1627f"
      },
      "outputs": [],
      "source": [
        "with open('foodmart.basket', 'r') as file:\n",
        "    for _ in range(10):\n",
        "        line = file.readline()\n",
        "        print(line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCy_M3c922aH"
      },
      "source": [
        "Consequently, we use pandas package to read the file into a DataFrame by splitting on STORE_ID_, resulting in two columns: ID and transaction record.\n",
        "\n",
        "Pandas is a powerful Python library for data manipulation and analysis, providing data structures like DataFrame and Series for efficient data handling and a wide range of tools for data cleaning, transformation, and analysis. You can get more information about pandas in this [link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "7yGYQJNE25vm",
        "outputId": "6d4efc4d-ac93-47ee-e4ce-f34b7009bc7d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'foodmart.basket'\n",
        "data = pd.read_csv(file_path, header=None, sep=', STORE_ID_', engine='python')\n",
        "data.columns = ['Transaction', 'ID']\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTtHxPOO33pY"
      },
      "source": [
        "Now let's analyze the transaction statistics with the aggregation function in DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "aVpfzFQj3AuQ",
        "outputId": "b15a18d0-e18a-44f4-b4aa-6a24b6a7b408"
      },
      "outputs": [],
      "source": [
        "transactions = data['Transaction'].apply(lambda x: x.split(','))\n",
        "\n",
        "cleaned_transactions = []\n",
        "for transaction in transactions:\n",
        "    cleaned_transaction = {}\n",
        "    for item in transaction:\n",
        "        if '=' in item:\n",
        "            name, count = item.split('=')\n",
        "            name = name.strip()\n",
        "            cleaned_transaction[name] = int(count)\n",
        "        elif item.strip() and item != 'STORE':\n",
        "            name = item.strip()\n",
        "            cleaned_transaction[name] = 1\n",
        "    if cleaned_transaction:\n",
        "        cleaned_transactions.append(cleaned_transaction)\n",
        "\n",
        "df = pd.DataFrame(cleaned_transactions).fillna(0)\n",
        "stats = df.agg(['mean', 'max', 'sum'])\n",
        "stats.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fDpXuaFBk8"
      },
      "source": [
        "Visualize the statistics of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TwlCU2gO5GLX",
        "outputId": "ca3833b9-f0f5-4f45-b001-d3e2f0eeddc2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "selected_stats = stats.sample(n=10, axis=1).T\n",
        "\n",
        "nrows = len(selected_stats.columns)\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(10, nrows * 4), squeeze=False)\n",
        "\n",
        "for i, column in enumerate(selected_stats.columns):\n",
        "    ax = axes[i, 0]\n",
        "    bars = selected_stats[column].plot(kind='bar', ax=ax, color='skyblue')\n",
        "    ax.set_title(f'Products by {column}')\n",
        "    ax.set_xlabel('Products')\n",
        "    ax.set_ylabel('Values')\n",
        "    ax.set_xticks(range(len(selected_stats.index)))\n",
        "    ax.set_xticklabels(selected_stats.index, rotation=45)\n",
        "\n",
        "    for bar in bars.patches:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate('{}'.format(height.round(2)),\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkoUDArIIc-K"
      },
      "source": [
        "###Filtering Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7NstNhoIfP9"
      },
      "source": [
        "The data filter function in Pandas allows you to select specific rows or columns based on certain conditions.\n",
        "\n",
        "You can filter rows by passing a boolean condition to the DataFrame. The condition should be a Series that matches the DataFrame's index.\n",
        "\n",
        "Here is an example we filt out rows with Milk greater than 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Xex6nzaTI89X",
        "outputId": "022d9505-65c0-49d9-d2a1-4f44f501d85e"
      },
      "outputs": [],
      "source": [
        "filtered_df = df[df['Milk'] > 2]\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS0xBE9mJzXu"
      },
      "source": [
        "##Frequent Itemset Mining and Association Rule Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-j2sRNJJ2mt"
      },
      "source": [
        "Frequent itemset mining is a data mining task to find frequent itemsets and association rules in a transaction database. It identifies the frequent individual items in a database and extends them to larger itemsets. The frequent itemsets determined by frequent itemset mining can be used to determine association rules which highlight general trends in the database. These rules can be used to identify products that are frequently bought together. For example, people who buy bread and eggs also tend to buy butter as well. Frequent itemset mining is usually used to mine association rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKv2FE-4K1yR"
      },
      "source": [
        " **Mlxtend**\n",
        "\n",
        " In this lab, we will use Mlxtend package to do the data mining. Mlxtend is an open-source Python library that extends the capabilities of data analysis and machine learning by providing additional tools for preprocessing, visualization, feature selection, model evaluation, and ensemble learning. It includes the Apriori algorithm for discovering frequent item sets and association rules in datasets, which is useful for market basket analysis and recommendation systems.  You can find more introductions and instructions about Mlxtend in [here](https://rasbt.github.io/mlxtend/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mlxtend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eImQOKg3vhPT"
      },
      "source": [
        "### Frequent Itemset Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5EayrQKMRku"
      },
      "source": [
        "We will use the apriori function to mine frequent itemsets. Minimum support is the minimum number of transactions that include an itemset. For example, the minimum support of 0.1 means that the itemset must appear in at least 10% of the transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6org4j4Nb3l"
      },
      "source": [
        "To use the apriori function, the data needs to be processed in the following format:\n",
        "\n",
        "A pandas DataFrame object, where each row represents a transaction, each column represents an item, and the value represents the number of times the item appears in that transaction (usually 0 or 1, indicating whether the item appears in the transaction).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "pZiPFmY3SniR",
        "outputId": "ef0c63b9-a89f-4e81-9f44-2805629d5b45"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "# give a document\n",
        "\n",
        "file_path = 'foodmart.basket'\n",
        "data = pd.read_csv(file_path, header=None, sep=', STORE_ID_', engine='python')\n",
        "\n",
        "transactions = data[0].apply(lambda x: x.split(','))\n",
        "\n",
        "cleaned_transactions = []\n",
        "for transaction in transactions:\n",
        "    cleaned_transaction = {}\n",
        "    for item in transaction:\n",
        "        if '=' in item:\n",
        "            name, count = item.split('=')\n",
        "            name = name.strip()\n",
        "            cleaned_transaction[name] = 1\n",
        "        elif item.strip() and item != 'STORE':\n",
        "            name = item.strip()\n",
        "            cleaned_transaction[name] = 1\n",
        "    if cleaned_transaction:\n",
        "        cleaned_transactions.append(cleaned_transaction)\n",
        "\n",
        "df = pd.DataFrame(cleaned_transactions).fillna(0)\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=0.0001, use_colnames=True, low_memory=True)\n",
        "\n",
        "frequent_itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWDtLp4vjWq"
      },
      "source": [
        "### Association Rule Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzvdilaSN7i0"
      },
      "source": [
        "The association_rules function from the mlxtend library is used to generate association rules from frequent item sets. This function can filter meaningful rules based on frequent item sets and given metrics such as confidence, lift, etc. Association rules describe relationships between item sets in the form of \"if A, then B,\" where A and B are item sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kcYXAiHPrfei",
        "outputId": "4c767d99-cdb1-4ee7-dc38-3906a263c6b6"
      },
      "outputs": [],
      "source": [
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.9, num_itemsets=len(frequent_itemsets))\n",
        "rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Fo2COpODxM"
      },
      "source": [
        "Instead of using the tools to analyze association rules directly, we can also use the large language model to mine for potential relationships. We can use the API to call the large model and pass in the extracted frequent_itemsets for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTazSFSJo0uj"
      },
      "source": [
        "You can generate your own APIKEY from [HKBU GenAI Platform](https://genai.hkbu.edu.hk/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_xjQDVkrQc6",
        "outputId": "ea6613aa-936b-47eb-88a3-3a2013e276f7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "apiKey = 'YOUR-API-KEY' # use your own API key\n",
        "basicUrl = \"https://genai.hkbu.edu.hk/general/rest\"\n",
        "modelName = \"gpt-4-o-mini\"\n",
        "apiVersion = \"2024-10-21\"\n",
        "\n",
        "def submit(message, df):\n",
        "    json_df = df.to_json(orient='records')\n",
        "\n",
        "    conversation = [\n",
        "        {\"role\": \"user\", \"content\": message + \" \" + json_df}\n",
        "    ]\n",
        "    url = basicUrl + \"/deployments/\" + modelName + \"/chat/completions/?api-version=\" + apiVersion\n",
        "    headers = {'Content-Type': 'application/json', 'api-key': apiKey}\n",
        "    payload = {'messages': conversation}\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data\n",
        "    else:\n",
        "        return 'Error:', response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmQIz6TuOglE"
      },
      "source": [
        "Here we took the top 100 most frequent Itemsets and designed a simple prompt. You can try passing in more data to dig deeper associations, or you can design a prompt yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjSFcZBXrdFT",
        "outputId": "d6bd7ffc-a788-4961-92ee-dd73de9fb119"
      },
      "outputs": [],
      "source": [
        "most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)[:100]\n",
        "prompt = 'I have collected data on the frequency of various shopping types of supermarket customers. Please help me analyze this data to find potential shopping relationships. Below are the categories and their respective frequencies:'\n",
        "result = submit(prompt, most_frequent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KlomL19CuUJa",
        "outputId": "ccfc84d9-3a0f-4bd3-b6ed-5df309252701"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "output = result['choices'][0]['message']['content']\n",
        "# print(output)\n",
        "display(Markdown(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Exercise: Amazon Review Data (2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpzZMEpIO6RE"
      },
      "source": [
        "Amazon review data (2018) is a large collection of reviews and metadata from Amazon products. The dataset contains 233.1 million reviews spanning May 1996 - Oct 2018. It contains reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014 for various products like books, electronics, movies, etc. This dataset is a slightly cleaned-up version of the data available at https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. The dataset is available in json format. We will be using the All_Beauty_5.json.gz file. Let's take a look at the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFmZbTT2PAVj"
      },
      "outputs": [],
      "source": [
        "!curl https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz -o All_Beauty_5.json.gz\n",
        "!gzip -d 'All_Beauty_5.json.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmXVYXQ0P4Db"
      },
      "source": [
        "Let's take a look at the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "{\n",
        "  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
        "  \"asin\": \"0000013714\",\n",
        "  \"reviewerName\": \"J. McDonald\",\n",
        "  \"helpful\": [2, 3],\n",
        "  \"reviewText\": \"I bought this for my husband who plays the piano. He is having a wonderful time playing these old hymns. The music is at times hard to read because we think the book was published for singing from more than playing from. Great purchase though!\",\n",
        "  \"overall\": 5.0,\n",
        "  \"summary\": \"Heavenly Highway Hymns\",\n",
        "  \"unixReviewTime\": 1252800000,\n",
        "  \"reviewTime\": \"09 13, 2009\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqCalekyQGXr"
      },
      "source": [
        "Since we are interested in mining frequent itemsets and association rules, we will only need the `reviewerID` and `asin` fields. The `reviewerID` field is the ID of the reviewer and the `asin` field is the ID of the product. We will be using these two fields to mine frequent itemsets and association rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_IIVRWATwh6"
      },
      "source": [
        "**Loading the products metadata from URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3YaBJfOQ44D",
        "outputId": "c5439b42-de6a-4d52-9581-a30f821aecdc"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "json_url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/metaFiles2/meta_All_Beauty.json.gz'\n",
        "\n",
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    return local_filename\n",
        "\n",
        "download_file(json_url)\n",
        "\n",
        "filename = json_url.split('/')[-1]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "with gzip.open(filename, 'rb') as f:\n",
        "    itemset = set()\n",
        "    for line in f:\n",
        "        record = json.loads(line)\n",
        "        row = pd.DataFrame([{\n",
        "            'asin': record['asin'],\n",
        "            'title': record['title'],\n",
        "            'description': record['description'],\n",
        "            'price': record['price'],\n",
        "            'brand': record['brand']\n",
        "        }])\n",
        "        df = pd.concat([df, row], ignore_index=True)\n",
        "filename = filename.split('.')[0]\n",
        "df.to_csv(f'{filename}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uRAEA8FT0yK"
      },
      "source": [
        "**Loading the Review Dataset for Top-5 Most Reviewed Products from URL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "SVXbXaBdRC_o",
        "outputId": "cdb83c92-8d31-4150-898e-4622a42550db"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "json_url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz'\n",
        "\n",
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    return local_filename\n",
        "\n",
        "\n",
        "download_file(json_url)\n",
        "filename = json_url.split('/')[-1]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "with gzip.open(filename, 'rb') as f:\n",
        "    itemset = set()\n",
        "    for line in f:\n",
        "        record = json.loads(line)\n",
        "        row = pd.DataFrame([{\n",
        "            'reviewerID': record['reviewerID'],\n",
        "            'asin': record['asin'],\n",
        "        }])\n",
        "        df = pd.concat([df, row], ignore_index=True)\n",
        "\n",
        "def export(x):\n",
        "    with open(f'{filename}.basket', 'a+b') as f:\n",
        "        dataline = f\"{df.at[x.index[0], 'reviewerID']}=1, {'=1,'.join(x.tolist())}=1\\n\"\n",
        "        f.write(dataline.encode('utf-8'))\n",
        "    return ','.join(x.tolist())\n",
        "\n",
        "df.groupby(by='reviewerID').agg(export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Ck6topTq1T"
      },
      "source": [
        "In above code, we download the top-5 most reviewed products All_Beauty_5.json.gz file by the download_file function. This function we have used in the previous lab. We then open the gzipped file and parse the json file line by line. We then extract the reviewerID and asin fields from the json file and store them in a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGzzCSytUoLS"
      },
      "source": [
        "Now, using the data obtained, complete the following exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH6LK_BRUpGp"
      },
      "source": [
        "#Exercise:\n",
        "\n",
        "\n",
        "1.   Find the most frequent itemsets with a minimum support of 0.001.\n",
        "2.   Find the association rules with a minimum support of 0.01 and a minimum confidence of 90%.\n",
        "3.   Filter out products with the generated association rule's antecedent.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
